{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c608d91b",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING\n",
    "\n",
    "### Author: Hyungjoo Kim\n",
    "\n",
    "##### All data was provided by University College London, Department of Computer Science, Reinforcement Learning Module\n",
    "\n",
    "##### Supervisor: Prof. Hado Van Hasselt, Matteo Hessel, and Diana Borsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e368ed5",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "In this assignment, we will take a first look at learning algorithms for sequential decision problems.\n",
    "\n",
    "**Background reading**\n",
    "\n",
    "* Sutton and Barto (2018), Chapters 3 - 6\n",
    "\n",
    "**Overview of this assignment**\n",
    "\n",
    "You will use Python to implement several reinforcement learning algorithms.\n",
    "\n",
    "You will then run these algorithms on a few problems, to understand their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901499a",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Import Useful Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2bfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900525bf",
   "metadata": {},
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46077464",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=1)\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86853c",
   "metadata": {},
   "source": [
    "### Some grid worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02ffe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = -100  # wall\n",
    "G = 100  # goal\n",
    "\n",
    "GRID_LAYOUT = np.array([\n",
    "  [W, W, W, W, W, W, W, W, W, W, W, W],\n",
    "  [W, W, 0, W, W, W, W, W, W, 0, W, W],\n",
    "  [W, 0, 0, 0, 0, 0, 0, 0, 0, G, 0, W],\n",
    "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
    "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
    "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
    "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
    "  [W, W, 0, 0, 0, 0, 0, 0, 0, 0, W, W],\n",
    "  [W, W, W, W, W, W, W, W, W, W, W, W]\n",
    "])\n",
    "\n",
    "class Grid(object):\n",
    "\n",
    "  def __init__(self, noisy=False):\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    self._layout = GRID_LAYOUT\n",
    "    self._start_state = (2, 2)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "\n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "      return self._number_of_states\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return y*self._layout.shape[1] + x\n",
    "\n",
    "  def obs_to_state(self, obs):\n",
    "    x = obs % self._layout.shape[1]\n",
    "    y = obs // self._layout.shape[1]\n",
    "    s = np.copy(grid._layout)\n",
    "    s[y, x] = 4\n",
    "    return s\n",
    "\n",
    "  def step(self, action):\n",
    "    y, x = self._state\n",
    "    \n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    reward = self._layout[new_y, new_x]\n",
    "    if self._layout[new_y, new_x] == W:  # wall\n",
    "      discount = 0.9\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
    "      reward = -1.\n",
    "      discount = 0.9\n",
    "    else:  # a goal\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "\n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 10*np.random.normal(0, width - new_x + new_y)\n",
    "\n",
    "    self._state = new_state\n",
    "    return reward, discount, self.get_obs()\n",
    "\n",
    "  def plot_grid(self):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != W, interpolation=\"nearest\", cmap='pink')\n",
    "    plt.gca().grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"The grid\")\n",
    "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    plt.text(9, 2, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26764c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_GRID_LAYOUT = np.array([\n",
    "  [W, W, W, W, W],\n",
    "  [W, W, 3, W, W],\n",
    "  [W, W, 0, W, W],\n",
    "  [W, 1, 0, W, W],\n",
    "  [W, W, 0, W, W],\n",
    "  [W, W, 0, 4, W],\n",
    "  [W, W, W, W, W]\n",
    "])\n",
    "\n",
    "\n",
    "def plot_small_grid(version=1):\n",
    "  plt.imshow(SMALL_GRID_LAYOUT < -1, interpolation='nearest', cmap='pink_r',\n",
    "             vmin=-0.2, vmax=1.2)\n",
    "  if version == 1:\n",
    "    plt.text(2, 1, '$+3$', ha='center', va='center', fontsize=12)\n",
    "    plt.text(1, 3, '$+1$', ha='center', va='center', fontsize=12)\n",
    "    plt.text(3, 5, '$+4$', ha='center', va='center', fontsize=12)\n",
    "  else:\n",
    "    plt.text(2, 1, '$+2$', ha='center', va='center', fontsize=12)\n",
    "    plt.text(1, 3, '$+1$', ha='center', va='center', fontsize=12)\n",
    "    plt.text(3, 5, '$+5$', ha='center', va='center', fontsize=12)\n",
    "  plt.text(2, 3, '$S$', ha='center', va='center', fontsize=12)\n",
    "  h, w = SMALL_GRID_LAYOUT.shape\n",
    "  for r in np.arange(0.5, h):\n",
    "    plt.plot([-0.5, w - 0.5], [r, r], '-k', lw=3, alpha=0.4)\n",
    "  for c in np.arange(0.5, w):\n",
    "    plt.plot([c, c], [-0.5, h - 0.5], '-k', lw=3, alpha=0.4)\n",
    "  plt.xticks([]); plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c38a1",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "(You should not have to change, or even look at, these.  Do run the cell to make sure the functions are loaded though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d2e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, number_of_steps):\n",
    "    mean_reward = 0.\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "    for i in range(number_of_steps):\n",
    "      reward, discount, next_state = grid.step(action)\n",
    "      action = agent.step(reward, discount, next_state)\n",
    "      mean_reward += reward\n",
    "    return mean_reward/float(number_of_steps)\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(grid, values, colormap='pink', vmin=0, vmax=10):\n",
    "  plt.imshow(values - 1000*(grid<0), interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ccfd6",
   "metadata": {},
   "source": [
    "# Implementing algorithms\n",
    "\n",
    "**Minimal agent interface**:\n",
    "\n",
    "Each agent should implement the following methods:\n",
    "\n",
    "### `__init__(self, number_of_actions, number_of_states, initial_observation, ...)`:\n",
    "The constructor will provide the agent the number of actions, number of states, and the initial state. You can get the initial state by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`. The constructor may accept additional arguments (denoted here by the ellipsis `...`) depending on the specific algorithm implemented by the agent.\n",
    "\n",
    "### `step(self, reward, discount, next_state)`:\n",
    "\n",
    "The step method of an agent should update its internals, in whatever way is appropriate for its learning algorithm, and return a new action to be executed in the environment.\n",
    "\n",
    "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_state` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_state})$\" in the update, because $\\gamma = 0$ (for whatever definition of $v$ is appropriate---for instance, $v(s)$ could be defined in terms of action values estimates that we are learning, for instance by $v(s) = \\max_a q(s, a)$).  Therefore, the end of an episode can be seamlessly handled with the same step function.\n",
    "\n",
    "**Note on the initial action**:\n",
    "\n",
    "Some algorithms (Q-learning, Sarsa) need to remember the last action in order to update its value when they see the next state and reward.  In the constructor (`__init__`), make sure you set the initial action to zero, e.g.,\n",
    "```\n",
    "def __init__(...):\n",
    "  (...)\n",
    "  self._last_action = 0\n",
    "  (...)\n",
    "```\n",
    "In our experiments the helper functions above will execute the action `0` (which corresponds to `up` in the grid world) as the initial action to begin the run loop of the experiment.  This initial action is only executed once, and the beginning of the very first episode---not at the beginning of each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4935bca",
   "metadata": {},
   "source": [
    "### A random agent\n",
    "\n",
    "Below we show a reference implementation of a simple random agent, implemented according to the interface above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9edc6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random(object):\n",
    "\n",
    "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
    "    self._number_of_actions = number_of_actions\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    next_action = np.random.randint(self._number_of_actions)\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310c783",
   "metadata": {},
   "source": [
    "### The grid\n",
    "\n",
    "The cell below shows the `Grid` environment that we will use in this section. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-100` for bumping into a wall, `+100` for reaching the goal, and `-1` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93f1e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = Grid()\n",
    "grid.plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fb6f4",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1 - Implement TD Learning\n",
    "Implement an agent that acts randomly, and _on-policy_ estimates state values $v(s)$, using one-step TD learning with step size $\\alpha=0.1$.\n",
    "\n",
    "In addition to the base interface, also implement a property `state_values(self)` returning the vector of all state values (one value per state).\n",
    "\n",
    "You should be able to use the `__init__` as provided below, so you just have to implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.  \n",
    "\n",
    "**Hint**: in the `step` you similarly will want to store the previous state to be able to compute the next TD error on the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f480fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTD(object):\n",
    "\n",
    "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
    "    self._values = np.zeros(number_of_states)\n",
    "    self._state = initial_state\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "\n",
    "  @property\n",
    "  def state_values(self):\n",
    "    return self._values\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    ''' Reference = Lecture 5: Model-Free Prediction page 28'''\n",
    "    # self._values[self._state] = q(s, a), which is state-action value function\n",
    "    self._values[self._state] = self._values[self._state] + self._step_size * (reward + discount * (self._values[next_state]) - self._values[self._state])\n",
    "    self._state = next_state  # update new state as the next state\n",
    "    \n",
    "    # This system implement randomly, so it's a random TD learning\n",
    "    next_action = np.random.randint(self._number_of_actions)\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a57ece",
   "metadata": {},
   "source": [
    "### Run the next cell to run the `RandomTD` agent on a grid world.\n",
    "\n",
    "If everything worked as expected, the plot below will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero (shown in orange below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28447971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this cell.\n",
    "agent = RandomTD(grid._layout.size, 4, grid.get_obs())\n",
    "run_experiment(grid, agent, int(1e5))\n",
    "v = agent.state_values\n",
    "plot_values(GRID_LAYOUT, v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-300, vmax=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452ff00",
   "metadata": {},
   "source": [
    "## Task 2 - Policy iteration \n",
    "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state and for each action we can look at the value of the resulting state, and would then pick the action with the highest reward plus subsequent state value. In other words, you can assume we can use $q(s, a) = \\mathbb{E}[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s, A_t = a]$, where $v$ is the value function learned by TD as implemented. Then we consider the policy that picks the action with the highest action value $q(s, a)$. You do **not** have to implement this, just answer the following question.\n",
    "\n",
    "The above amounts to performing an iteration of policy evaluation and policy improvement.  If we repeat this process over and over again, and repeatedly evaluate the greedy policy and then perform an improvement step by picking the greedy policy, would the policy eventually become optimal?  Explain why or why not in at most three sentences.\n",
    "\n",
    "> In the policy iteration of policy evaluation and improvement situation, there is a possibility that the greedy policy can continue to select an action if the average reward gradually increases but not significantly. This is because the agent does not properly recognise the value of the reward where it is a higher reward in some cases. Therefore, the greedy policy does not take into account exploration so that the policy is eventually not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851510d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
