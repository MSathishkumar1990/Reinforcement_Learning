{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8ac36e",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING\n",
    "\n",
    "### Author: Hyungjoo Kim\n",
    "\n",
    "##### All data was provided by University College London, Department of Computer Science, Reinforcement Learning Module\n",
    "\n",
    "##### Supervisor: Prof. Hado Van Hasselt, Matteo Hessel, and Diana Borsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46202be9",
   "metadata": {},
   "source": [
    "### Mathematical Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77a2e6",
   "metadata": {},
   "source": [
    "### Task 1\n",
    " Consider a MDP $M = (\\mathcal{S}, \\mathcal{A}, p, r, \\gamma)$ and a behaviour policy $\\mu$. We use policy $\\mu$ to generate trajectories of experience:\n",
    "\\begin{equation*}\n",
    "    (s_{t}, a_{t}, r_{t},s_{t+1}, a_{t+1}, r_{t+1},\\cdots, s_{t+n-1}, a_{t+n-1}, r_{t+n-1}, s_{t+n}, a_{t+n}) \\,.\n",
    "\\end{equation*}\n",
    "Note that this is an $n$-step sequence, starting from time $t$.\n",
    "\n",
    "Given these partial trajectories we consider the following learning problems:\n",
    "\n",
    "### Task 1.1\n",
    "Consider a learning update based on the following temporal difference error:\n",
    "    \\begin{equation}\n",
    "        \\delta_t = r(s_{t},a_{t}) + \\gamma  r(s_{t+1},a_{t+1}) + \\gamma^2 \\max_{a} q(s_{t+2},a) - q(s_t, a_t)\n",
    "    \\end{equation}\n",
    "\n",
    "Consider updating a tabular action value function with TD. \n",
    "* i) Does the resulting value function converge, under any initialisation of the value function? Consider an appropiate learning rate (Robbinsâ€“Monro conditions). If so, prove the convergence under infinity number of interactions with this MDP, under behaviour policy $\\mu$. If not, show what it converges to instead, or show why it diverges. \n",
    "\n",
    "> According to the papers [1] [2], convergence is demonstrated using temporal difference (TD) learning for control. Update the policy rather than finding the value of the current policy to ensure that the optimal policy is reached. As a result, these studies have demonstrated the convergence of 2 steps returns of Q-learning. If convergence is wanted to guarantee under any initialisation of it, the several conditions should be satisfied. \n",
    ">\n",
    "> **Theorem under the following assumptions:**\n",
    "> - $0 \\leq \\alpha_t \\leq 1$, $\\sum_t \\alpha_t (x) = \\infty$ and $\\sum_t \\alpha^{2}_{t}(x) \\lt \\infty$ , which considers an appropriate learning rate using Robbins-Monro.\n",
    "> - $||\\mathbb{E}[F_t(s) | \\mathcal{F}_t]||_W \\leq \\gamma ||\\Delta_t||_W$, where $\\gamma \\in (0, 1) \\ and \\ w\\rightarrow \\infty$\n",
    "> - $var[F_t(s) | \\mathcal{F}_t] \\leq K(1 + ||\\Delta_t||^{2}_{W})$, for $K \\gt 0 \\ and \\ w\\rightarrow \\infty$\n",
    ">\n",
    "> Firstly, following the Q-learning update rule can be represented as follows:\n",
    ">\n",
    "> $$ q_{t+1}(s_t, a_t) = q(s_t, a_t) + \\alpha_t(s_t, a_t) \\left[ r(s_{t},a_{t}) + \\gamma  r(s_{t+1},a_{t+1}) + \\gamma^2 \\max_{a} q(s_{t+2},a) - q(s_t, a_t) \\right] $$\n",
    ">\n",
    "> Let's subtracting from the left and right side of equation the quantity $q^*(s_t, a_t)$\n",
    ">\n",
    "> $$ \\Delta_t(s, a) = q_t(s, a) - q^*(s, a) $$\n",
    ">\n",
    "> we can derive $\\Delta_t(s_t, a_t)$ using the Q-learning update rule as follows:\n",
    ">\n",
    "> $$ \\Delta_t(s_t, a_t) = (1 - \\alpha_t(s_t, a_t))\\Delta_t(s_t, a_t) + \\alpha_t(s_t, a_t) \\left[r(s_{t},a_{t}) + \\gamma  r(s_{t+1},a_{t+1}) + \\gamma^2 \\max_{a} q(s_{t+2},a) - q^{*}_{\\pi}(s_t, a_t) \\right] $$\n",
    ">\n",
    "> From now on, the above theorem under the following assumptions comes from the random process $(\\Delta_t)$ taking values in $\\mathbb{R}^n$ and defined as $\\Delta_{t+1}(s) = (1-\\alpha_t(s))\\Delta_t(s) + \\alpha_t(s)F_t(s)$ converges to zero with probability 1.\n",
    ">\n",
    ">, where $F_t(s) = r(s_{t},a_{t}) + \\gamma  r(s_{t+1},a_{t+1}) + \\gamma^2 \\max_{a} q(s_{t+2},a) - q^{*}_{\\pi}(s_t, a_t)$, and $q^*(s_t, a_t)$ is the fixed point, so the optimal Q-function is a fixed point of a contraction operator $\\mathcal{T}^\\pi$.\n",
    ">\n",
    "> Secondly,$p$ represents the transition probabilities, and $r$ represents the reward function based on the MDP. $\\mathbb{E}[F_t(s) | \\mathcal{F}]$ can be derived using the transition probabilities under the behaviour policy $\\mu$ as:\n",
    ">\n",
    ">$$ F_t(s) = r(s_{t},a_{t}, X(s, a) + \\gamma  r(s_{t+1},a_{t+1}, X(s_{t+1},a_{t+1})) + \\gamma^2 \\max_{a} q(s_{t+2},a) - q^{*}_{\\pi}(s_t, a_t) $$\n",
    ">\n",
    ">, where $X(s, a)$ and $X(s_{t+1},a_{t+1})$ are a random sample state obtained from the Markov chain ($S, p_a$)\n",
    ">\n",
    ">$$ \\mathbb{E}[F_t(s) | \\mathcal{F}_t] = \\sum_{s \\in \\mathcal{A}} p_a(s, a) \\left[r(s_{t},a_{t}) + \\gamma  r(s_{t+1},a_{t+1}) + \\gamma^2 \\max_{a} q(s_{t+2},a) - q^{*}_{\\pi}(s_t, a_t) \\right] $$\n",
    ">\n",
    ">, where $\\sum_{s \\in \\mathcal{A}} p_a(s, a) = \\sum_{s_{t+1}}p(s_{t+1}|s_t, a_t) \\sum_{a_{t+1}}\\mu(a_{t+1}|s_{t+1}) \\sum_{s_{t+2}}p(s_{t+2}|s_{t+2}, a_{t+1}) $\n",
    ">\n",
    "> For simplicity using the approximation $q^* = \\mathcal{T}^\\pi q^*$, which is based on the property of Bellman equation,\n",
    ">\n",
    ">$$ \\mathbb{E}[F_t(s) | \\mathcal{F}_t] = \\sum_{s \\in \\mathcal{A}} p_a(s, a) F_t(s) = \\mathcal{T}^\\pi q(s_t, a_t) - q^{*}_{\\pi}(s_t, a_t)$$\n",
    ">\n",
    ">$$ \\mathbb{E}[F_t(s) | \\mathcal{F}_t] = \\mathcal{T}^\\pi q(s_t, a_t) - \\mathcal{T}^\\pi q^{*}_{\\pi}(s_t, a_t)$$\n",
    ">\n",
    "> Under infinity number of interactions with the MDP,\n",
    ">\n",
    "> $$ ||\\mathbb{E}[F_t(s) | \\mathcal{F}_t]||_{\\infty} \\leq \\gamma ||q_t - q^{*}_{\\pi}||_{\\infty} = \\gamma ||\\Delta_t||_{\\infty} $$\n",
    ">\n",
    "> Thirdly,\n",
    ">\n",
    ">$$ var[F_t(s) | \\mathcal{F}_t] = \\mathbb{E} \\left[ (r(s_{t},a_{t}, X(s, a)) + \\gamma  r(s_{t+1},a_{t+1}, X(s_{t+1},a_{t+1})) + \\gamma^2 \\max_{a} q(s_{t+2},a) - q^{*}_{\\pi}(s_t, a_t) - \\mathcal{T}^\\pi q(s, a) + q^*(s, a))^2 \\right]$$\n",
    ">\n",
    ">$$ = \\mathbb{E} \\left[ (r(s_{t},a_{t}, X(s, a)) + \\gamma  r(s_{t+1},a_{t+1}, X(s_{t+1},a_{t+1})) + \\gamma^2 \\max_{a} q(s_{t+2},a) - \\mathcal{T}^\\pi q(s, a))^2 \\right]$$\n",
    ">\n",
    ">$$ = var\\left[ (r(s_{t},a_{t}, X(s, a)) + \\gamma  r(s_{t+1},a_{t+1}, X(s_{t+1},a_{t+1})) + \\gamma^2 \\max_{a} q(s_{t+2},a) | \\mathcal{F}_t \\right]$$\n",
    ">\n",
    ">, where the reward $r$ is bounded.\n",
    ">\n",
    "> $$ \\therefore var[F_t(s) | \\mathcal{F}_t] \\leq K(1 + ||\\Delta_t||^2_{\\infty}) $$\n",
    ">\n",
    ">, where this condition is under the infinity number of the interactions with the MDP and $K$ is for some constant.\n",
    ">\n",
    "> Based on the above three following principles and the theorem under the following assumptions, $\\Delta_t$ is guaranteed to converge to zero with probability 1, which means the value function converges under any initialisation of the value function when the Robbins-Monro conditions are considered.\n",
    ">\n",
    ">[1] H. V. Hasselt and M. A. Wiering, \"Convergence of Model-Based Temporal Difference Learning for control\", *Intelligent Systems Group, Department of Information and Computing Sciences*, Utrecht University, Netherlands, 2007.\n",
    ">\n",
    ">[2] F. S. Melo, \"Convergence of Q-learning: A Simple Proof\", *Institute for Systems and Robotics*, Portugal.\n",
    "\n",
    "* ii) Under which conditions, would the above process converge to the optimal value function $q_*$? \n",
    "\n",
    "> According to the paper [3], if we want to find the optimal value function that satisfy the Bellman equation, then the optimal policy should be found firstly. For evaluating these principles, we need to use the policy evaluation method (e.g. policy iteration). In the MDP transition process to evaluate the policy, a couple of conditions should be satisfied [3] [4] [5].\n",
    "> - **(1)** Acting greedy policy should be used for satisfying the Bellman operator or the Bellman expectation equation\n",
    "> - **(2)** Evaluating a final policy (decision policy) that all state-action pairs should be explored under an infinity number of times.\n",
    ">\n",
    "> Firstly, the acting greedy represents picking an action, which is the maximum value in a particular state as:\n",
    ">\n",
    "> $$ \\pi^{'}(s) = argmax_{a \\in A} q_\\pi(s, a) $$\n",
    ">\n",
    ">A paper [5] argues that taking the first step greedy $\\pi^{'}(s)$. After that, following the policy $\\pi$ can be equal or better than following the policy all along. It can be represented as follows:\n",
    ">\n",
    ">$$ q_\\pi (s, \\pi^{'}(s)) = max_{a \\in A} q_\\pi(s, a) \\ge q_\\pi(s, \\pi(s)) = v_\\pi(s) $$\n",
    ">\n",
    "> In other words, if the greedy policy $\\pi^{'}(s)$ at the first step is applied and then following the policy $\\pi$, then the policy evaluation becomes better and better, so it can find the optimal policy under an infinity number of times **(2)** when the evaluation is stopped. After that, the Bellman optimal equation can be satisfied **(1)** if the value function $v_\\pi(s)$ is the same as the greedy value function $q_\\pi (s, \\pi^{'}(s))$.\n",
    ">\n",
    "> $$ v_\\pi(s) = max_{a \\in A} q_\\pi(s, a) $$\n",
    ">\n",
    "> Therefore, the final policy $\\pi$ is the decision policy, which is the optimal policy when the conditions are satisfied following the above principles. Finally, the value function is guaranteed to converge its optimal value function using the optimal policy.\n",
    ">\n",
    "> [3] R. S. Sutton and A. G. Barto, \"Reinforcement Learning: An Introduction - Dynamic Programming\", *The MIT Press*, Cambridge, Massachusetts, London, pp 73-90.\n",
    ">\n",
    "> [4] H. V. Hasselt, D. Borsa and M. Hessel, \"COMP0089 Lecture 6: Model-Free Control\" *University College London*, Department of Computer Science, London, 2021.\n",
    ">\n",
    "> [5] Blackburn, \"Reinforcement Learning: Solving Markov Decision Process using Dynamic Programming\", *towards data science*, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c40efc",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Consider the same questions now for the following temporal difference error\n",
    "\\begin{equation}\n",
    "        \\delta_t = r(s_{t},a_{t}) + \\gamma \\frac{\\pi(a_{t+1}|s_{t+1})}{\\mu(a_{t+1}|s_{t+1})} \\left[ r(s_{t+1},a_{t+1}) + \\gamma \\max_{a} q(s_{t+2},a) \\right] - q(s_t, a_t)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\pi(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$ and consider the behaviour policy to be: a) $\\mu(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$, b) $\\mu(a|s) = \\frac{1}{|\\mathcal{A}|}$ (uniformly random policy).\n",
    "\n",
    "Answer the below two questions for **both choices** of the behaviour policy $\\mu$:\n",
    "* i)  Does updating a tabular action value function with this TD error converge to the optimal value function $q_*$? Consider an appropiate learning rate (Robbinsâ€“Monro conditions). If so, prove this convergence under infinity number of interaction with this MDP, under behaviour policy $\\mu$. If not, show why it diverges. \n",
    "\n",
    "> **Case a):** $\\mu(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$\n",
    ">\n",
    "> If we apply the following **case (a)**, which is the greedy policy, then it would be similar to question 5.1.i). This is because acting greedy would not explore sufficiently under the infinity number of interaction with the MDP and a couple of conditions from question 5.1.ii) would not satisfied, so the case (a) eventually converges to its fixed point, but not to the optimal value function. \n",
    ">\n",
    "> **Case b):** $\\mu(a|s) = \\frac{1}{|\\mathcal{A}|}$ (uniformly random policy)\n",
    ">\n",
    "> Conversely, if we apply the following **case (b)**, which is the uniformly random policy, then it would converge to the optimal value function. This is because the following uniformly random policy would explore sufficiently under the infinity number of interaction with the MDP, so using the policy iteration method can properly find the optimal policy. This can be represented as follows:\n",
    ">\n",
    "> **Theorem under the following assumptions:**\n",
    "> - $0 \\leq \\alpha_t \\leq 1$, $\\sum_t \\alpha_t (x) = \\infty$ and $\\sum_t \\alpha^{2}_{t}(x) \\lt \\infty$ , which considers an appropriate learning rate using Robbins-Monro.\n",
    "> - $||\\mathbb{E}[F_t(s) | \\mathcal{F}_t]||_W \\leq \\gamma ||\\Delta_t||_W$, where $\\gamma \\in (0, 1) \\ and \\ w\\rightarrow \\infty$\n",
    "> - $var[F_t(s) | \\mathcal{F}_t] \\leq K(1 + ||\\Delta_t||^{2}_{W})$, for $K \\gt 0 \\ and \\ w\\rightarrow \\infty$\n",
    ">\n",
    "> Firstly, we need to consider an appropriate learning rate using Robbins-Monro conditions and the Q-learning update rule can be represented as:\n",
    ">\n",
    ">$$ q_{t+1}(s_t, a_t) = q(s_t, a_t) + \\alpha_t(s_t, a_t) \\left[ r(s_{t},a_{t}) + \\gamma \\frac{\\pi(a_{t+1}|s_{t+1})}{\\mu(a_{t+1}|s_{t+1})} \\left[ r(s_{t+1},a_{t+1}) + \\gamma \\max_{a} q(s_{t+2},a) \\right] - q(s_t, a_t) \\right] $$\n",
    ">\n",
    "> Secondly, we can use the optimal value function as mentioned in the above principles for using the **case (b)**, so $F_t(s)$ can be written as follows:\n",
    ">$$F_t(s) = r(s_{t},a_{t}, X(s, a)) + \\gamma \\frac{\\pi(a_{t+1}|s_{t+1})}{\\mu(a_{t+1}|s_{t+1})} \\left[ r(s_{t+1},a_{t+1}, X(s_{t+1}, a_{t+1})) + \\gamma \\max_{a} q(s_{t+2},a) \\right] - q_{*}(s_t, a_t)$$\n",
    ">\n",
    ">, where $X(s, a)$ and $X(s_{t+1},a_{t+1})$ are a random sample state obtained from the Markov chain ($S, p_a$), and also $q_{*}(s_t, a_t)$ is the optimal value function, which is satisfying the Bellman operator **(1) from Q5.1.ii)**, not the fixed point. This is because all state-action pairs are explored under an infinity number of times due to using uniformly random sampling **(2) from Q5.1.ii)** and finally find the optimal value function.   \n",
    ">\n",
    "> Thirdly, the third condition of theroem can be satisfied that the reward is continuously bounded under the infinity number of time steps. \n",
    ">\n",
    "> All of the above principles can be proved to converge to the optimal value function in **case (b)** using the same method of question 5.1 when the Robbins-Monro conditions are considered. Therefore, updating the tabular action-value function with this TD error based on the theorem under the policy **(b)** converges to the optimal value function.\n",
    "\n",
    "\n",
    "* ii) How does the variance of this update compare to the one induced by the error in Q5.1?  \n",
    "\n",
    "> **Case a):** $\\mu(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S}$\n",
    ">\n",
    ">Policy (a) is the greedy policy as mentioned in the above question 5.1. Let's imagine the Q-learning algorithm from the above temporal difference equation, which includes $\\max_{a}q(s_{t+2}, a)$. If the Q-learning estimates the following the value of the greedy policy, the acting greedy would not explore sufficiently all the time so that we can imagine the variance will be not better. Furthermore, this can be similar to question 5.1.i) because the importance sampling term does not need to compute anymore due to choosing the greedy policy all the time. \n",
    ">\n",
    "> **Case b):** $\\mu(a|s) = \\frac{1}{|\\mathcal{A}|}$ (uniformly random policy)\n",
    ">\n",
    ">However, policy (b) is the uniformly random policy, which means we do not need greedy behaviour anymore. In fact, Q-learning can learn the optimal value function eventually even when you explore uniformly random throughout the infinite number of times. In other words, the Q-learning learns for any policy that eventually selects all actions sufficiently, such as the uniform random policy. Therefore, we can imagine that the variance will be better. \n",
    ">\n",
    "> Thus, the variance with the following policy (b) is better than the following policy (a) and Q5.1, moreover, the variance for the case (a) and Q5.1 would be similar.\n",
    ">\n",
    "\n",
    "* iii) Can you propose a different behaviour policy that achieves a lower variance than any of the choices we consider for $\\mu$? Prove that your behaviour policy achieve this. Argue why, if that is not possible.\n",
    "\n",
    "> The epsilon-greedy selection can be said to belong to the uniformly random policy. The uniformly random policy has the disadvantage of choosing equally among all actions, although it is an effective means of balancing exploration and exploitation in reinforcement learning. In other words, it means that it is as likely to select the worst behaviour as it is to select the next best option. The softmax policy selection method [6] can be used to change the action probabilities as a graded function of the estimated value. Therefore, while the highest probability is chosen for the greedy actions, all remaining actions can be ranked and weighted according to value estimates so that the disadvantage can be removed. Thus, if the behaviour policy uses the softmax policy method, then the variance can be reduced.\n",
    ">\n",
    "> The softmax policy selection can be represented as follows [7]:\n",
    ">\n",
    ">$$ \\mu(a) = \\frac{e^{H_t (a)}} {\\sum_b e^{H_t (b)}}$$\n",
    ">\n",
    ">, where $H_t (a)$ is an action preferences, which are not values and they are learnable policy parameters.\n",
    ">\n",
    "> One more suggestion is that the value of the following temporal difference error equation can make smaller if the middle term ($\\gamma \\frac{\\pi(a_{t+1}|s_{t+1})}{\\mu(a_{t+1}|s_{t+1})} \\left[ r(s_{t+1},a_{t+1}) + \\gamma \\max_{a} q(s_{t+2},a) \\right]$) is smaller or even zero.  This is because we can control the behaviour policy, which means the importance sampling can be controlled to reduce the value of the middle term. Therefore, we can suggest that the behaviour policy choose a minimum value with respect to the value function.\n",
    ">\n",
    ">$$ \\mu(a|s) \\in argmin_a q(s, a), \\forall s,a \\in \\mathcal{A} \\times \\mathcal{S} $$\n",
    ">\n",
    "> Therefore, the middle term will be smaller under the infinite number of time steps and finally become zero because of \n",
    ">$$\\frac{\\pi(a_{t+1}|s_{t+1})}{\\mu(a_{t+1}|s_{t+1})} = \\frac{argmax_a q(s, a)}{argmin_a q(s, a)} \\approx 0$$\n",
    ">\n",
    "> The following TD error would be\n",
    ">\n",
    "> $$ \\delta_t = r(s_t, a_t) - q(s_t, a_t) $$\n",
    ">\n",
    "> In the above equation, the first reward has remained and it will be bounded by the first reward. Thus, if this behaviour policy applies to the following TD, then the value of TD error would be decreased and also can achieve a lower variance.\n",
    ">\n",
    "> [6]  R. S. Sutton and A. G. Barto, \"Reinforcement Learning: An Introduction - Dynamic Programming\", The MIT Press, Cambridge, Massachusetts, London, pp 37-45.\n",
    ">\n",
    "> [7] H. V. Hasselt, D. Borsa and M. Hessel, \"COMP0089 Lecture 2: Exploration and Exploitation\" *University College London*, Department of Computer Science, London, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b91e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
